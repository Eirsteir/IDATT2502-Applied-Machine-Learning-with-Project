{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LTSM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QiunL9nUdUZ",
        "outputId": "966ff8a4-b23c-4212-d059-2f72ce00cc7f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(dev)\n",
        "\n",
        "\n",
        "class LongShortTermMemoryModel(nn.Module):\n",
        "    def __init__(self, encoding_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(encoding_size, 128)  # 128 is the state size\n",
        "        self.dense = nn.Linear(128, encoding_size)  # 128 is the state size\n",
        "\n",
        "    def reset(self):  # Reset states prior to new input sequence\n",
        "        zero_state = torch.zeros(1, 1, 128)  # Shape: (number of layers, batch size, state size)\n",
        "        self.hidden_state = zero_state\n",
        "        self.cell_state = zero_state\n",
        "\n",
        "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
        "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state.to(dev), self.cell_state.to(dev)))\n",
        "        return self.dense(out.reshape(-1, 128)).to(dev)\n",
        "\n",
        "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
        "        return torch.softmax(self.logits(x), dim=1)\n",
        "\n",
        "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
        "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1)).to(dev)\n",
        "\n",
        "\n",
        "encoding_size = len(char_encodings)\n",
        "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
        "char_encodings = np.eye(len(index_to_char))\n",
        "\n",
        "x_train = torch.tensor([[char_encodings[0]], [char_encodings[1]], [char_encodings[2]], [char_encodings[3]], [char_encodings[3]], [char_encodings[4]], \n",
        "                       [char_encodings[0]], [char_encodings[5]], [char_encodings[4]], [char_encodings[6]], [char_encodings[3]], [char_encodings[7]]], dtype=torch.float).to(dev)  # ' hello world'\n",
        "y_train = torch.tensor([char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0], \n",
        "                       char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]], dtype=torch.float).to(dev)  # 'hello world'\n",
        "\n",
        "model = LongShortTermMemoryModel(encoding_size).to(dev)\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
        "for epoch in range(500):\n",
        "    model.reset()\n",
        "    model.loss(x_train.to(dev), y_train.to(dev)).backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        # Generate characters from the initial characters ' h'\n",
        "        model.reset()\n",
        "        text = ' h'\n",
        "        model.f(torch.tensor([[char_encodings[0]]], dtype=torch.float).to(dev))\n",
        "        y = model.f(torch.tensor([[char_encodings[1]]], dtype=torch.float).to(dev))\n",
        "        text += index_to_char[y.argmax(1)]\n",
        "        for c in range(50):\n",
        "            y = model.f(torch.tensor([[char_encodings[y.argmax(1)]]], dtype=torch.float).to(dev))\n",
        "            text += index_to_char[y.argmax(1)]\n",
        "        print(text)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            " hllloo                                              \n",
            " hlllo wrll                                          \n",
            " hlllo world    dd    d                              \n",
            " hello world   rld    rdd   rld   rdd    rdd   rld   \n",
            " hello world   rld   rld   rrdd   rld   rld   rld   r\n",
            " hello world  wrld   rld   rld   rrld  wrld  wrld   r\n",
            " hello world  wrld  wrld  wrld  wrld  wrld  wrrdd  rr\n",
            " hello world  wrld  wrld  wrld  wrld  wrld  wrld  wrr\n",
            " hello world  wrld  wrld  wrld  wrld  wrld  wrld  wrr\n",
            " hello world  wrld  wrld  wrld  wrld  wrld  wrld  wrl\n",
            " hello world  wrld  wrld  wrld  wrld  wrld  wrld  wrl\n",
            " hello world world  wrld  wrld  wrld  wrld  wrld  wrl\n",
            " hello world world world  wrld  wrld  wrld  wrld  wrl\n",
            " hello world world world world  wrld  wrld  wrld  wrl\n",
            " hello world world world world world world  wrld  wrl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n",
            " hello world world world world world world world worl\n"
          ]
        }
      ]
    }
  ]
}